---
layout: default
description: Contents
---
[(논문 요약) Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data](./gemma2.html)

[(논문 요약) Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data](./connectdots.html)

[(논문 요약) WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting](./workbench.html)

[(논문 요약) Meta Large Language Model Compiler: Foundation Models of Compiler Optimization](./metacompiler.html)

[(논문 요약) TREE SEARCH FOR LANGUAGE MODEL AGENTS](./treellmagent.html)

[(논문 요약) Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step by Step](./ldb.html)

[(논문 요약) DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](./deepseekcoderv2.html)

[(논문 요약) AGENTGYM: Evolving Large Language Model-based Agents across Diverse Environments](./agentgym.html)

[(논문 요약) MAGPIE: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](./magpie.html)

[(논문 요약) Mixture-of-Agents Enhances Large Language Model Capabilities](./togethermoa.html)

[(논문 요약) OpenVLA: An Open-Source Vision-Language-Action Model](./openvla.html)

[(논문 요약) Show, Don’t Tell: Aligning Language Models with Demonstrated Feedback](./ditto.html)

[(논문 요약) Towards Scalable Automated Alignment of LLMs: A Survey](./alignment.html)

[(논문 요약) Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](./bot.html)

[(논문 요약) SimPO: Simple Preference Optimization with a Reference-Free Reward](./simpo.html)

[(논문 요약) Faithful Logical Reasoning via Symbolic Chain-of-Thought](./symboliccot.html)

[(논문들 요약) Large Language Model Tuning](./llmtune.html)

[(논문 요약) Extreme Compression of Large Language Models via Additive Quantization](./aqlm.html)

[(논문 요약) QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](./quip.html)

[(논문 요약) DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data](./deepseek-prover.html)

[(논문 요약) Layer-Condensed KV Cache for Efficient Inference of Large Language Models](./lckv.html)

[(논문 요약) Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](./sonnetinterpret.html)

[(코드 탐색) llama.cpp](./llamacpp.html)

[(논문 요약) Granite Code Models: A Family of Open Foundation Models for Code Intelligence](./granite.html)

[(논문 요약) Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](./finetuning-hallucination.html)

[(논문 요약) Chameleon: Mixed-Modal Early-Fusion Foundation Models](./chameleon.html)

[(논문 요약) What matters when building vision-language models?](./idefics2.html)

[(코드 실행) Alphacodium](./alphacodium-code.html)

[(논문 요약) Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](./alphacodium.html)

[(논문 요약) Better & Faster Large Language Models via Multi-token Prediction](./multitoken.html)

[(논문 요약) LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](./llmint8.html)

[(논문 요약) OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](./OBELICS.html)

[(논문 요약) SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING](./swe-agent.html)

[(논문 요약) Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages](./chatvector.html)

[(논문 요약) AgentCoder: Multiagent-Code Generation with Iterative Testing and Optimisation](./agentcoder.html)

[(논문 요약) NExT: Teaching Large Language Models to Reason about Code Execution](./next.html)

[(논문 요약) Make Your LLM Fully Utilize the Context](./utilize-context.html)

[(논문 요약) Phi-3 Technical Report:A Highly Capable Language Model Locally on Your Phone](./phi3.html)

[(데이터 요약) common crawl filtered data](./web-crawl-data.html)

[(논문 요약) One Embedder, Any Task: Instruction-Finetuned Text Embeddings](./instructor-embedding.html)

[(모델 요약) LLaMa3](./llama3.html)

[(논문 요약) CodeGemma: Open Code Models Based on Gemma](./codegemma.html)

[(논문 요약) RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](./recurrentgemma.html)

[(논문 요약) Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](./infiniattn.html)

[(논문 요약) Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study](./rdr2.html)

[(논문 요약) Integrating Code Generation with Execution and Refinement](./opencode.html)

[(논문 요약) Gecko: Versatile Text Embeddings Distilled from Large Language Models](./gecko.html)

[(논문 요약) Gorilla: Large Language Model Connected with Massive APIs](./gorilla.html)

[(논문 요약) The Unreasonable Ineffectiveness of the Deeper Layers](./ineffectlayer.html)

[(논문 요약) SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?](./swebench.html)

[(논문 요약) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](./galore.html)

[(논문 요약) RAFT: Adapting Language Model to Domain Specific RAG](./raft.html)

[(논문 요약) LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS](./safe.html)

[(논문 요약) MTEB: Massive Text Embedding Benchmark](./mteb.html)

[(논문 요약) Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs](./chart-based-reasoning.html)

[(논문 요약) Generative Representational Instruction Tuning](./gen_rep_inst.html)

[(논문 요약) Text Embeddings by Weakly-Supervised Contrastive Pre-training](./e5.html)

[(논문 요약) Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](./branch-train-mix.html)

[(논문 요약) Direct Preference Optimization: Your Language Model is Secretly a Reward Model](./dpo.html)

[(논문 요약) RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](./rat.html)

[(논문 요약) Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](./quiet-star.html)

[(논문 요약) STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning](./star-self-taught.html)

[(논문 요약) SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model](./scenescript.html)

[(논문 요약) DEMYSTIFYING EMBEDDING SPACES USING LARGE LANGUAGE MODELS](./cav.html)

[(논문 요약) Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](./stopregress.html)

[(논문 요약) Scaling Instructable Agents Across Many Simulated Worlds](./sima.html)

[(논문 요약) Simple and Scalable Strategies to Continually Pre-train Large Language Models](./contllm.html)

[(논문 요약) Efficient Tool Use with Chain-of-Abstraction Reasoning](./coa.html)

[(논문 요약) Beyond A\*: Better Planning with Transformers via Search Dynamics Bootstrapping](./searchformer.html)

[(논문 요약) InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](./internlm-math.html)

[(논문 요약) Grounded Language-Image Pre-training](./glip.html)

[(코드 실행) AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model](./anymal.html)

[(논문 요약) Textbooks Are All You Need](./textbook.html)

[(논문 요약) Generating Diverse High-Fidelity Images with VQ-VAE-2](./vqvae2.html)

[(논문 요약) Neural Discrete Representation Learning](./vqvae.html)

[FED 금리 수익률 곡선](./fed_interest.html)

[Undistort Image with OpenCV-Python](./undistort.html)

[Top-view perspective transform with OpenCV-Python](./topview_transformation.html)

[Rotation matrix with Quaternion](./quaternion.html)

[Visualize a point cloud with Open3D](./pointcloud.html)

