---
layout: default
---
# All pages (in chronological order)
<br>

[(논문 요약) The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers](./docs/Language%20Model/Application/copilot/)

[(논문 요약) Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation](./docs/Language%20Model/Hallucination/strategicCoT/)

[(논문 요약) Open Mixture-of-Experts Language Models](./docs/Language%20Model/Foundation%20Model/openmoe/)

[(논문 요약) DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](./docs/Language%20Model/Foundation%20Model/deepseekv2/)

[(모델 요약) Yi-Coder](./docs/Language%20Model/Data/yicoder/)

[(모델 요약) Reflection Llama](./docs/Language%20Model/Data/reflectionllama/)

[(논문 요약) DIFFUSION MODELS ARE REAL-TIME GAME ENGINES](./docs/Computer%20Vision/gameengine)

[(논문 요약) Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](./docs/Vision%20Language%20Model/transfusion)

[(논문 요약) Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](./docs/Language%20Model/Training/smaller)

[(논문 요약) To Code, or Not To Code? Exploring Impact of Code in Pre-training](./docs/Language%20Model/Code%20and%20Math/tocode)

[(논문 요약) Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](./docs/Language%20Model/RAG/speculativeRAG)

[(논문 요약) Scaling Laws for Data Filtering—Data Curation cannot be Compute Agnostic](./docs/Language%20Model/Training/datafiltering)

[(논문 요약) LLM Pruning and Distillation in Practice: The Minitron Approach](./docs/Language%20Model/Compute%20Efficiency/llmpractice)

[(논문 요약) Automated Design of Agentic Systems](./docs/Language%20Model/Agents/autoagent)

[(논문 요약) xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](./docs/Vision%20Language%20Model/blip3)

[(논문 요약) MUTUAL REASONING MAKES SMALLER LLMS STRONGER PROBLEM-SOLVERS](./docs/Language%20Model/Code%20and%20Math/mutualreasoning)

[(논문 요약) MEDICAL GRAPH RAG: TOWARDS SAFE MEDICAL LARGE LANGUAGE MODEL VIA GRAPH RETRIEVALAUGMENTED GENERATION](./docs/Language%20Model/RAG/medicalgraphrag)

[(논문 요약) Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](./docs/Language%20Model/Application/medprompt)

[(논문 요약) Distributed Inference and Fine-tuning of Large Language Models Over The Internet](./docs/Language%20Model/Distributed%20Training/petal)

[(논문 요약) Self-Taught Evaluators](./docs/Language%20Model/Training/ste)

[(논문 요약) Scaling Exponents Across Parameterizations and Optimizers](./docs/Language%20Model/Training/scaling)

[(논문 요약) Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](./docs/Language%20Model/Code%20and%20Math/llmonkey)

[(논문 요약) REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS](./docs/Language%20Model/Code%20and%20Math/react)

[(논문 요약) Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget](./docs/Computer%20Vision/microbudget)

[(논문 요약) MindSearch: Mimicking Human Minds Elicits Deep AI Searcher](./docs/Language%20Model/Agents/mindsearch)

[(논문 요약) Apple Intelligence Foundation Language Models](./docs/Language%20Model/Foundation%20Model/afm)

[(논문 요약) SAM 2: Segment Anything in Images and Videos](./docs/Computer%20Vision/sam2)

[(논문 요약) Segment Anything](./docs/Computer%20Vision/sam)

[(논문 요약) META-REWARDING LANGUAGE MODELS: Self-Improving Alignment with LLM-as-a-Meta-Judge](./docs/Language%20Model/Training/metareward)

[(논문 요약) LazyLLM: DYNAMIC TOKEN PRUNING FOR EFFICIENT LONG CONTEXT LLM INFERENCE](./docs/Language%20Model/Compute%20Efficiency/lazyllm)

[(논문 요약) MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens](./docs/Language%20Model/Data/mint)

[(논문 요약) Solving olympiad geometry without human demonstrations](./docs/Language%20Model/Code%20and%20Math/alphageometry)

[(논문 요약) LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover](./docs/Language%20Model/Code%20and%20Math/leangithub)

[(논문 요약) Weak-to-Strong Reasoning](./docs/Language%20Model/Code%20and%20Math/weaktostrong)

[(논문 요약) PROVER-VERIFIER GAMES IMPROVE LEGIBILITY OF LLM OUTPUTS](./docs/Language%20Model/Hallucination/pvgame)

[(논문 요약) The Llama 3 Herd of Models](./docs/Language%20Model/Foundation%20Model/llama31)

[(논문 요약) From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](./docs/Language%20Model/Compute%20Efficiency/welore)

[(논문 요약) Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](./docs/Language%20Model/Compute%20Efficiency/qgalore)

[(논문 요약) DataComp-LM: In search of the next generation of training sets for language models](./docs/Language%20Model/Data/datacomp)

[(논문 요약) Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](./docs/Computer%20Vision/florence)

[(논문 요약) FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](./docs/Language%20Model/Compute%20Efficiency/flashattention3)

[(논문 요약) RouteLLM: Learning to Route LLMs with Preference Data](./docs/Language%20Model/Training/routellm)

[(논문 요약) INTERNET OF AGENTS: WEAVING A WEB OF HETEROGENEOUS AGENTS FOR COLLABORATIVE INTELLIGENCE](./docs/Language%20Model/Agents/internetagents)

[(논문 요약) Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages](./docs/Language%20Model/Data/planetarium)

[(논문 요약) DiPaCo: Distributed Path Composition](./docs/Language%20Model/Distributed%20Training/dipaco)

[(논문 요약) Data curation via joint example selection further accelerates multimodal learning](./docs/Language%20Model/Data/jest)

[(논문 요약) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](./docs/Language%20Model/Compute%20Efficiency/mobilellm)

[(논문 요약) APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets](./docs/Language%20Model/Data/apigen)

[(논문 요약) Searching for Best Practices in Retrieval-Augmented Generation](./docs/Language%20Model/RAG/bestRAG)

[(논문 요약) AGENTLESS: Demystifying LLM-based Software Engineering Agents](./docs/Language%20Model/Code%20and%20Math/agentless)

[(논문 요약) InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output](./docs/Language%20Model/Foundation%20Model/internlm)

[(논문 요약) Gemma 2: Improving Open Language Models at a Practical Size](./docs/Language%20Model/Foundation%20Model/gemma2)

[(논문 요약) Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data](./docs/Language%20Model/Analysis/connectdots)

[(논문 요약) WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting](./docs/Language%20Model/Data/workbench)

[(논문 요약) Meta Large Language Model Compiler: Foundation Models of Compiler Optimization](./docs/Language%20Model/Code%20and%20Math/metacompiler)

[(논문 요약) TREE SEARCH FOR LANGUAGE MODEL AGENTS](./docs/Language%20Model/Agents/treellmagent)

[(논문 요약) Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step by Step](./docs/Language%20Model/Agents/ldb)

[(논문 요약) DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](./docs/Language%20Model/Code%20and%20Math/deepseekcoderv2)

[(논문 요약) AGENTGYM: Evolving Large Language Model-based Agents across Diverse Environments](./docs/Language%20Model/Agents/agentgym)

[(논문 요약) MAGPIE: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](./docs/Language%20Model/Training/magpie)

[(논문 요약) Mixture-of-Agents Enhances Large Language Model Capabilities](./docs/Language%20Model/Agents/togethermoa)

[(논문 요약) OpenVLA: An Open-Source Vision-Language-Action Model](./docs/Vision%20Language%20Model/openvla)

[(논문 요약) Show, Don’t Tell: Aligning Language Models with Demonstrated Feedback](./docs/Language%20Model/Alignment/ditto)

[(논문 요약) Towards Scalable Automated Alignment of LLMs: A Survey](./docs/Language%20Model/Alignment/alignment)

[(논문 요약) Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](./docs/Language%20Model/Code%20and%20Math/bot)

[(논문 요약) SimPO: Simple Preference Optimization with a Reference-Free Reward](./docs/Language%20Model/Training/simpo)

[(논문 요약) Faithful Logical Reasoning via Symbolic Chain-of-Thought](./docs/Language%20Model/Code%20and%20Math/symboliccot)

[(논문들 요약) Large Language Model Tuning](./docs/Language%20Model/Training/llmtune)

[(논문 요약) Extreme Compression of Large Language Models via Additive Quantization](./docs/Language%20Model/Compute%20Efficiency/aqlm)

[(논문 요약) QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](./docs/Language%20Model/Compute%20Efficiency/quip)

[(논문 요약) DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data](./docs/Language%20Model/Code%20and%20Math/deepseek-prover)

[(논문 요약) Layer-Condensed KV Cache for Efficient Inference of Large Language Models](./docs/Language%20Model/Compute%20Efficiency/lckv)

[(논문 요약) Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](./docs/Language%20Model/Analysis/sonnetinterpret)

[(코드 탐색) llama.cpp](./docs/Code%20Review/llamacpp)

[(논문 요약) Granite Code Models: A Family of Open Foundation Models for Code Intelligence](./docs/Language%20Model/Code%20and%20Math/granite)

[(논문 요약) Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](./docs/Language%20Model/Hallucination/finetuning-hallucination)

[(논문 요약) Chameleon: Mixed-Modal Early-Fusion Foundation Models](./docs/Vision%20Language%20Model/chameleon)

[(논문 요약) What matters when building vision-language models?](./docs/Vision%20Language%20Model/idefics2)

[(코드 실행) Alphacodium](./docs/Language%20Model/Code%20and%20Math/alphacodium-code)

[(논문 요약) Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](./docs/Language%20Model/Code%20and%20Math/alphacodium)

[(논문 요약) Better & Faster Large Language Models via Multi-token Prediction](./docs/Language%20Model/Compute%20Efficiency/multitoken)

[(논문 요약) LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](./docs/Language%20Model/Compute%20Efficiency/llmint8)

[(논문 요약) OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](./docs/Language%20Model/Data/OBELICS)

[(논문 요약) SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING](./docs/Language%20Model/Agents/swe-agent)

[(논문 요약) Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages](./docs/Language%20Model/Training/chatvector)

[(논문 요약) AgentCoder: Multiagent-Code Generation with Iterative Testing and Optimisation](./docs/Language%20Model/Agents/agentcoder)

[(논문 요약) NExT: Teaching Large Language Models to Reason about Code Execution](./docs/Language%20Model/Code%20and%20Math/next)

[(논문 요약) Make Your LLM Fully Utilize the Context](./docs/Language%20Model/Training/utilize-context)

[(논문 요약) Phi-3 Technical Report:A Highly Capable Language Model Locally on Your Phone](./docs/Language%20Model/Foundation%20Model/phi3)

[(데이터 요약) common crawl filtered data](./docs/Language%20Model/Data/web-crawl-data)

[(논문 요약) One Embedder, Any Task: Instruction-Finetuned Text Embeddings](./docs/Language%20Model/Embedding/instructor-embedding)

[(모델 요약) LLaMa3](./docs/Language%20Model/Foundation%20Model/llama3)

[(논문 요약) CodeGemma: Open Code Models Based on Gemma](./docs/Language%20Model/Code%20and%20Math/codegemma)

[(논문 요약) RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](./docs/Language%20Model/Compute%20Efficiency/recurrentgemma)

[(논문 요약) Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](./docs/Language%20Model/Architecture/infiniattn)

[(논문 요약) Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study](./docs/Language%20Model/Application/rdr2)

[(논문 요약) Integrating Code Generation with Execution and Refinement](./docs/Language%20Model/Code%20and%20Math/opencode)

[(논문 요약) Gecko: Versatile Text Embeddings Distilled from Large Language Models](./docs/Language%20Model/Embedding/gecko)

[(논문 요약) Gorilla: Large Language Model Connected with Massive APIs](./docs/Language%20Model/Agents/gorilla)

[(논문 요약) The Unreasonable Ineffectiveness of the Deeper Layers](./docs/Language%20Model/Compute%20Efficiency/ineffectlayer)

[(논문 요약) SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?](./docs/Language%20Model/Agents/swebench)

[(논문 요약) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](./docs/Language%20Model/Compute%20Efficiency/galore)

[(논문 요약) RAFT: Adapting Language Model to Domain Specific RAG](./docs/Language%20Model/RAG/raft)

[(논문 요약) LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS](./docs/Language%20Model/Hallucination/safe)

[(논문 요약) MTEB: Massive Text Embedding Benchmark](./docs/Language%20Model/Data/mteb)

[(논문 요약) Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs](./docs/Vision%20Language%20Model/chart-based-reasoning)

[(논문 요약) Generative Representational Instruction Tuning](./docs/Language%20Model/Training/gen_rep_inst)

[(논문 요약) Text Embeddings by Weakly-Supervised Contrastive Pre-training](./docs/Language%20Model/Embedding/e5)

[(논문 요약) Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](./docs/Language%20Model/Architecture/branch-train-mix)

[(논문 요약) Direct Preference Optimization: Your Language Model is Secretly a Reward Model](./docs/Language%20Model/Training/dpo)

[(논문 요약) RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](./docs/Language%20Model/RAG/rat)

[(논문 요약) Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](./docs/Language%20Model/Hallucination/quiet-star)

[(논문 요약) STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning](./docs/Language%20Model/Hallucination/star-self-taught)

[(논문 요약) SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model](./docs/Language%20Model/Application/scenescript)

[(논문 요약) DEMYSTIFYING EMBEDDING SPACES USING LARGE LANGUAGE MODELS](./docs/Language%20Model/Analysis/cav)

[(논문 요약) Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](./docs/Reinforcement%20Learning/stopregress)

[(논문 요약) Scaling Instructable Agents Across Many Simulated Worlds](./docs/Language%20Model/Agents/sima)

[(논문 요약) Simple and Scalable Strategies to Continually Pre-train Large Language Models](./docs/Language%20Model/Training/contllm)

[(논문 요약) Efficient Tool Use with Chain-of-Abstraction Reasoning](./docs/Language%20Model/Code%20and%20Math/coa)

[(논문 요약) Beyond A\*: Better Planning with Transformers via Search Dynamics Bootstrapping](./docs/Language%20Model/Application/searchformer)

[(논문 요약) InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](./docs/Vision%20Language%20Model/internlm-math)

[(논문 요약) Grounded Language-Image Pre-training](./docs/Language%20Model/Training/glip)

[(코드 실행) AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model](./docs/Vision%20Language%20Model/anymal)

[(논문 요약) Textbooks Are All You Need](./docs/Language%20Model/Training/textbook)

[(논문 요약) Generating Diverse High-Fidelity Images with VQ-VAE-2](./docs/Computer%20Vision/vqvae2)

[(논문 요약) Neural Discrete Representation Learning](./docs/Computer%20Vision/vqvae)

[(논문 요약) ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING](./docs/Language%20Model/Embedding/rope)

[FED 금리 수익률 곡선](./docs/Economy/fed_interest)

[Undistort Image with OpenCV-Python](./docs/Computer%20Vision/undistort)

[Top-view perspective transform with OpenCV-Python](./docs/Computer%20Vision/topview_transformation)

[Rotation matrix with Quaternion](./docs/Computer%20Vision/quaternion)

[Visualize a point cloud with Open3D](./docs/Computer%20Vision/pointcloud)

